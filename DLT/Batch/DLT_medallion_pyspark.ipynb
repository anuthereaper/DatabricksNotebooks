{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f39c84a-10a4-4117-8cc9-3a8e4feaf0c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# THIS DLT PIPELINE DEMOS HOW TO LOAD A DLT TABLE USING AUTOLOADER\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from datetime import datetime\n",
    "import dlt\n",
    "\n",
    "# get spark config for the table1 and table2\n",
    "table1 = spark.conf.get(\"table1\", \"cust1\")\n",
    "table2 = spark.conf.get(\"table2\", \"order1\")\n",
    "catalog = spark.conf.get(\"catalog\", \"jri\")\n",
    "schema = spark.conf.get(\"schema\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4037c3db-3ed0-4d4d-9c2a-7b1d400bd5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    " # tag/table name      name              constraint\n",
    " (\"bronze_cust1\",  \"correct_schema\", \"_rescued_data IS NULL\"),\n",
    " (\"bronze_order1\",  \"correct_schema\", \"_rescued_data IS NULL\"),\n",
    " (\"silver_cust1\",  \"valid_id\",       \"customer_id IS NOT NULL AND customer_id > 0\"),\n",
    " (\"silver_order1\", \"valid_order_id\", \"order_id IS NOT NULL AND order_id > 0\"),\n",
    " (\"silver_order1\", \"valid_cust_id\",  \"customer_id IS NOT NULL AND customer_id > 0\")\n",
    "]\n",
    "#Typically only run once, this doesn't have to be part of the DLT pipeline.\n",
    "spark.createDataFrame(data=data, schema=[\"tag\", \"name\", \"constraint\"]).write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.expectations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87f88e7d-aaaf-4341-90e5-f2e7c507f05b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Return the rules matching the tag as a format ready for DLT annotation.\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "def get_rules(tag):\n",
    "  \"\"\"\n",
    "    loads data quality rules from csv file\n",
    "    :param tag: tag to match\n",
    "    :return: dictionary of rules that matched the tag\n",
    "  \"\"\"\n",
    "  rules = {}\n",
    "  df = spark.table(f\"{catalog}.{schema}.expectations\").where(f\"tag = '{tag}'\")\n",
    "  for row in df.collect():\n",
    "    rules[row['name']] = row['constraint']\n",
    "  return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0f51e6-57a9-4378-b272-64e755f8726c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=f\"bronze_{table1}\",temporary=False)\n",
    "#@dlt.expect_all_or_fail({\"valid columns\" , \"col(“A”) = col(“B”)\"})\n",
    "#@dlt.expect_all_or_fail({\"valid_columns\": \"A = B\"})\n",
    "@dlt.expect_all_or_drop(get_rules(f'bronze_{table1}')) #get the rules from our centralized table.\n",
    "def incremental_view():\n",
    "    return(spark.readStream.format(\"cloudFiles\") \\\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"abfss://schema@jridatalakesng.dfs.core.windows.net/bronze_{table1}/\") \\\n",
    "        .option(\"cloudFiles.format\", \"csv\") \\\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "        .option(\"cloudFiles.includeExistingFiles\",\"true\") \\\n",
    "        .load(f\"abfss://raw@jridatalakesng.dfs.core.windows.net/{table1}/\"))\n",
    "    \n",
    "@dlt.table(name=f\"bronze_{table2}\",temporary=False)\n",
    "@dlt.expect_all_or_drop(get_rules(f'bronze_{table2}')) #get the rules from our centralized table.\n",
    "#@dlt.expect_all_or_fail({\"valid columns\" , \"col(“A”) = col(“B”)\"})\n",
    "#@dlt.expect_all_or_fail({\"valid_columns\": \"A = B\"})\n",
    "def incremental_view():\n",
    "    return(spark.readStream.format(\"cloudFiles\") \\\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"abfss://schema@jridatalakesng.dfs.core.windows.net/bronze_{table2}/\") \\\n",
    "        .option(\"cloudFiles.format\", \"csv\") \\\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "        .option(\"cloudFiles.includeExistingFiles\",\"true\") \\\n",
    "        .load(f\"abfss://raw@jridatalakesng.dfs.core.windows.net/{table2}/\"))\n",
    "\n",
    "@dlt.table(name=f\"silver_{table1}\")\n",
    "@dlt.expect_all_or_drop(get_rules(f'silver_{table1}')) #get the rules from our centralized table.\n",
    "def incremental_silver():\n",
    "    df  = spark.sql(f\"SELECT customer_id,name,mobile_number,A,B FROM STREAM LIVE.BRONZE_{table1}\")\n",
    "    return(df)\n",
    "\n",
    "@dlt.table(name=f\"silver_{table2}\")\n",
    "@dlt.expect_all_or_drop(get_rules(f'silver_{table2}')) #get the rules from our centralized table.\n",
    "def incremental_silver():\n",
    "    df  = spark.sql(f\"SELECT order_id,customer_id,product_id,cob FROM STREAM LIVE.BRONZE_{table2}\")\n",
    "    return(df)\n",
    "\n",
    "@dlt.table(name=f\"gold_aggr\")\n",
    "def incremental_gold():\n",
    "    df  = spark.sql(f\"SELECT A.customer_id, order_id,product_id,cob,name,mobile_number  FROM STREAM LIVE.SILVER_{table1} A LEFT JOIN LIVE.SILVER_{table2} B on A.customer_id = B.customer_id;\")\n",
    "    return(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT_medallion_pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
