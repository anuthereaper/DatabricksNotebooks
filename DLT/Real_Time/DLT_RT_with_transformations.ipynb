{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ca8b461-a189-4b0e-8e4a-130b5152d43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import dlt\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "#from pyspark.sql.types import *\n",
    "binary_to_string = fn.udf(lambda x: str(int.from_bytes(x, byteorder='big')), StringType())\n",
    "\n",
    "def get_config(filegroup, layer):\n",
    "    \"\"\"\n",
    "        loads data quality rules from a table\n",
    "        :param tag: tag to match\n",
    "        :return: dictionary of rules that matched the tag\n",
    "    \"\"\"\n",
    "    config_list = [] \n",
    "    #df = spark.read.table(\"config\")\n",
    "    query = f\"SELECT * FROM rt_config WHERE target_layer = \\\"{layer}\\\" and filegroup_id = {filegroup}\"\n",
    "    df = spark.sql(query)\n",
    "\n",
    "    input_list = [] \n",
    "    for row in df.collect():\n",
    "        config = {}\n",
    "        sql_stmt = row['sql_stmt']\n",
    "        config[\"kafka_server\"] = row['kafka_server']\n",
    "        config[\"kafka_key\"] = row['kafka_key']\n",
    "        config[\"kv_kafka_secret_key\"] = row['kv_kafka_secret_key']\n",
    "        config[\"out_table\"] = row['out_table']\n",
    "        config[\"topic\"] = row['topic']\n",
    "\n",
    "        dq_rules = {}\n",
    "        for i in row[\"dq_rules\"]:\n",
    "            dq_rules = {i[\"name\"] : i[\"expectation\"]}\n",
    "            #dq_rules.append(dq_rule)\n",
    "        \n",
    "        config[\"dq_rules\"] = dq_rules\n",
    "\n",
    "        inputs_json = json.loads(row['inputs'])\n",
    "        input_list = [] \n",
    "        for key in inputs_json:\n",
    "            input_list.append(inputs_json[key])\n",
    "            if row['out_type'] == \"TABLE\":\n",
    "                table_name = inputs_json[key]\n",
    "            else:\n",
    "                table_name = inputs_json[key].split('.')[-1]\n",
    "            sql_stmt = sql_stmt.replace(\"{\" + key + \"}\", \"LIVE.\" + table_name)     #The table name is split to remove catalog and schema as this a temporary view\n",
    "        config[\"input_tables\"] = input_list\n",
    "        config[\"sql_stmt\"] = sql_stmt\n",
    "        \n",
    "        config_list.append(config)\n",
    "\n",
    "    return config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb8be3a2-f67d-4a06-ab53-c431536b6ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_table_to_staging(kafka_server, kafka_key, kv_kafka_secret_key, out_table, topic):\n",
    "    @dlt.table(name=f\"STG_{out_table}\",comment=f\"This is a Staging table STG_{out_table}\",)\n",
    "    def real_time_stage():\n",
    "        kafka_secret = dbutils.secrets.get(scope=\"jriscopekv\", key=kv_kafka_secret_key)\n",
    "        sasl = f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_key}\" password=\"{kafka_secret}\";'\n",
    "\n",
    "        kafka_options = {\n",
    "            \"kafka.bootstrap.servers\": kafka_server,\n",
    "            \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "            \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "            \"kafka.request.timeout.ms\": \"60000\",\n",
    "            \"kafka.session.timeout.ms\": \"30000\",\n",
    "            \"startingOffsets\": \"earliest\",\n",
    "            \"kafka.sasl.jaas.config\": sasl,\n",
    "            \"subscribe\": topic\n",
    "        }\n",
    "        rtdf =  spark.readStream.format(\"kafka\").options(**kafka_options).load().withColumn('key', fn.col(\"key\").cast(StringType())).withColumn('actualvalue', fn.col(\"value\").cast(StringType())).withColumn(\"load_date\",lit(datetime.now()))\n",
    "\n",
    "        return rtdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b19dd1-a63f-4149-8c4b-d3bb164066e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stage_loading = get_config(3,\"STAGE\")\n",
    "for config in stage_loading:\n",
    "    load_table_to_staging(config[\"kafka_server\"],config[\"kafka_key\"],config[\"kv_kafka_secret_key\"], config[\"out_table\"], config[\"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c63fb8-1666-457c-b64d-7a494d93c87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=f\"catalog\",comment=f\"This is a dim catalog table\",)\n",
    "def catalog_silver():\n",
    "    df =  spark.sql(\"SELECT * FROM STREAM catalog\")\n",
    "    return df\n",
    "\n",
    "@dlt.table(name=f\"SILVER_TXN\",comment=f\"This is the formatted silver transaction table\",)\n",
    "def transaction_silver():\n",
    "    formattxn_df =  spark.sql(\"SELECT v.* from (SELECT from_json(cast(value AS STRING), 'ordertime BIGINT, orderid INT, itemid STRING, billamount INT, country STRING') v FROM STREAM LIVE.STG_Txn)\")\n",
    "    return formattxn_df\n",
    "\n",
    "@dlt.table(name=f\"SILVER_JOIN\",comment=f\"This is a Joined table\",)\n",
    "def stream_static_join():\n",
    "    rtdf =  spark.sql(\"SELECT A.itemid,A.ordertime, A.orderid, B.item_desc, A.billamount,A.country FROM STREAM LIVE.SILVER_TXN as A INNER JOIN STREAM LIVE.catalog as B ON A.itemid = B.itemid;\")\n",
    "    return rtdf\n",
    "\n",
    "\n",
    "@dlt.table(name=f\"GOLD_AGG\",comment=f\"This is an aggregated table\",)\n",
    "def aggregations():\n",
    "    rtdf =  spark.sql(\"SELECT COUNT(*) AS COUNT, country FROM LIVE.SILVER_TXN GROUP BY country;\")\n",
    "    return rtdf"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT_RT_with_transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
