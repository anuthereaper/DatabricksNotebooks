{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5d2fe5-4972-4688-87de-cb27cba99708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"src_table\",\"default_src\")\n",
    "dbutils.widgets.text(\"target_table\",\"default_target\")\n",
    "dbutils.widgets.text(\"sql_group\",\"default_id\")\n",
    "dbutils.widgets.text(\"db_name\",\"default\")\n",
    "dbutils.widgets.text(\"mode\",\"default\")\n",
    "dbutils.widgets.text(\"hostname\",\"default\")\n",
    "dbutils.widgets.text(\"key_scope\",\"default\")\n",
    "dbutils.widgets.text(\"cred_type\",\"default\")\n",
    "dbutils.widgets.text(\"userid\",\"default\")\n",
    "dbutils.widgets.text(\"password\",\"default\")\n",
    "dbutils.widgets.text(\"tenantId\",\"tenant_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1e9150-4320-4389-8ed5-7dc48402c2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_table = dbutils.widgets.get(\"src_table\")\n",
    "target_table = dbutils.widgets.get(\"target_table\")\n",
    "sql_group = dbutils.widgets.get(\"sql_group\")\n",
    "db_name = dbutils.widgets.get(\"db_name\")\n",
    "mode = dbutils.widgets.get(\"mode\")\n",
    "hostname = dbutils.widgets.get(\"hostname\")\n",
    "key_scope = dbutils.widgets.get(\"key_scope\")\n",
    "cred_type = dbutils.widgets.get(\"cred_type\")\n",
    "userid = dbutils.widgets.get(\"userid\")\n",
    "password = dbutils.widgets.get(\"password\")\n",
    "tenantId = dbutils.widgets.get(\"tenantId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b314d3-268c-4ea7-b6fd-c37b0b4ce480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "storagename= \"jridatalakesng\"\n",
    "tempdir = \"abfss://synapsestaging@\" + storagename + \".dfs.core.windows.net/mydir\"\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"{datetime.now()}: INFO : Table : {target_table}. Processing started. {start_time}\")\n",
    "\n",
    "try:\n",
    "    # Reading as a stream ensures that only the new data is pickedup. It also ensures easy restartability incase of failure.\n",
    "    spark.readStream \\\n",
    "        .option(\"skipChangeCommits\", \"True\") \\\n",
    "        .table(src_table) \\\n",
    "        .createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "    df = spark.sql(f\"SELECT * FROM temp_view\")\n",
    "\n",
    "    #WRITE DATA WITH OVERWRITE\n",
    "    dbtable_new = f\"dbo.{target_table}\"\n",
    "    if cred_type == \"useridpassword\":\n",
    "        query_stream = df.writeStream \\\n",
    "            .format(\"com.databricks.spark.sqldw\") \\\n",
    "            .option(\"host\", hostname) \\\n",
    "            .trigger(availableNow=True) \\\n",
    "            .option(\"port\", \"1433\") \\\n",
    "            .option(\"user\", dbutils.secrets.get(scope=key_scope, key=userid)) \\\n",
    "            .option(\"password\", dbutils.secrets.get(scope=key_scope, key=password)) \\\n",
    "            .option(\"database\", db_name) \\\n",
    "            .option(\"dbtable\", target_table) \\\n",
    "            .option(\"mode\",mode) \\\n",
    "            .option(\"tempDir\", tempdir) \\\n",
    "            .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
    "            .option(\"checkpointLocation\", f\"abfss://checkpoint@{storagename}.dfs.core.windows.net/synapsewrite/{target_table}\") \\\n",
    "            .start()\n",
    "    else:\n",
    "        query_stream = df.writeStream \\\n",
    "            .format(\"com.databricks.spark.sqldw\") \\\n",
    "            .option(\"host\", hostname) \\\n",
    "            .trigger(availableNow=True) \\\n",
    "            .option(\"port\", \"1433\") \\\n",
    "            .option(\"tenantId\", tenantId) \\\n",
    "            .option(\"clientId\", dbutils.secrets.get(scope=key_scope, key=userid)) \\\n",
    "            .option(\"clientSecret\", dbutils.secrets.get(scope=key_scope, key=password)) \\\n",
    "            .option(\"database\", db_name) \\\n",
    "            .option(\"dbtable\", target_table) \\\n",
    "            .option(\"mode\",mode) \\\n",
    "            .option(\"tempDir\", tempdir) \\\n",
    "            .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
    "            .option(\"checkpointLocation\", f\"abfss://checkpoint@{storagename}.dfs.core.windows.net/synapsewrite/{target_table}\") \\\n",
    "            .start()\n",
    "\n",
    "    print(f\"{datetime.now()}: INFO : Stream id is : {query_stream.id}\")\n",
    "    print(f\"{datetime.now()}: INFO : Stream run id is : {query_stream.runId}\")\n",
    "\n",
    "    # Wait for stream write to terminate before reporting back status\n",
    "    query_stream.awaitTermination()\n",
    "    end_time = datetime.now()\n",
    "\n",
    "    raw_data = query_stream.lastProgress\n",
    "    row_cnt = raw_data['numInputRows']\n",
    "    completion_status = \"Success\"\n",
    "except Exception as e:\n",
    "    print(f\"{datetime.now()}: ERROR : {e}\")\n",
    "    completion_status = \"Failed\"\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "print(f\"{datetime.now()}: INFO : Row_count {row_cnt}\")\n",
    "print(f\"{datetime.now()}: INFO : Table {target_table} completed. {end_time}\")\n",
    "print(f\"{datetime.now()}: INFO : time taken {time_taken}\")\n",
    "#spark.streams.resetTerminated() # Otherwise awaitAnyTermination() will return immediately after first stream has terminated\n",
    "#spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4500fd7-68b2-4fea-9841-02494bf447f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "exit_msg = json.dumps({\"sql_group\":sql_group, \"source_table\": src_table, \"target_table\": target_table, \"status\": completion_status, \"time_taken\": str(time_taken),\"load_start_ts\":str(start_time), \"load_end_ts\": str(end_time), \"rows_written\": row_cnt, \"stream_runid\" : query_stream.runId, \"stream_id\": query_stream.id})\n",
    "\n",
    "dbutils.notebook.exit(exit_msg)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Child_notebook",
   "widgets": {
    "sql_group": {
     "currentValue": "sql_group",
     "nuid": "1bde047c-aec5-497a-a28f-013d1eb6dee9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default_id",
      "label": null,
      "name": "sql_group",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default_id",
      "label": null,
      "name": "sql_group",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "src_table": {
     "currentValue": "table_25mb",
     "nuid": "916b8e5b-27bb-440c-b260-835eef7d5b1f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default_src",
      "label": null,
      "name": "src_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default_src",
      "label": null,
      "name": "src_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_table": {
     "currentValue": "table_25mba",
     "nuid": "e5a59778-ca7c-40b2-b366-1512c48e8ff0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default_target",
      "label": null,
      "name": "target_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default_target",
      "label": null,
      "name": "target_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
