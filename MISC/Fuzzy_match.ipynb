{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b32d96-7dc7-455c-82f1-b52a99ce7e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install faker \n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f58a563-5363-4809-975f-a01c000b33b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"ref count\",\"100000\")\n",
    "dbutils.widgets.text(\"input count\",\"10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a35b135-8495-43ed-8c38-4303a16bd426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "row_no = int(dbutils.widgets.get(\"ref count\"))\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import uuid\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "import random\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data_df  = RandomRDDs.uniformVectorRDD(spark, row_no,1).map(lambda a : a.tolist()).toDF()    # row, column\n",
    "\n",
    "fake = Faker('en_US')\n",
    "fake1 = Faker('en_GB')   # To generate phone numbers\n",
    "\n",
    "start = datetime.datetime.utcnow()\n",
    "print(\"Starting preparation: \", start.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "# Adding an id column with unique values\n",
    "nameUdf = udf(lambda : str(fake.name()),StringType())\n",
    "\n",
    "data_df = data_df.withColumn(\"ref_name\", nameUdf())\n",
    "\n",
    "data_df = data_df.drop(\"_1\")\n",
    "\n",
    "data_df.write.format(\"delta\").saveAsTable(\"DIM_CUST\")\n",
    "\n",
    "end = datetime.datetime.utcnow()\n",
    "print(\"Completed : \", end.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "print(\"Time taken : \", (end - start))\n",
    "spark.sql(\"SELECT * FROM DIM_CUST\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "730b8971-d921-4739-830f-6b8b9aaf20d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "input_no = int(dbutils.widgets.get(\"input count\"))\n",
    "\n",
    "data_df  = RandomRDDs.uniformVectorRDD(spark, input_no,1).map(lambda a : a.tolist()).toDF()    # row, column\n",
    "\n",
    "fake = Faker('en_US')\n",
    "fake1 = Faker('en_GB')   # To generate phone numbers\n",
    "\n",
    "start = datetime.datetime.utcnow()\n",
    "print(\"Starting preparation: \", start.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "# Adding an id column with unique values\n",
    "nameUdf = udf(lambda : str(fake.name()),StringType())\n",
    "\n",
    "data_df = data_df.withColumn(\"name\", nameUdf())\n",
    "\n",
    "data_df = data_df.drop(\"_1\")\n",
    "\n",
    "data_df.write.format(\"delta\").saveAsTable(\"v_input_df\")\n",
    "\n",
    "end = datetime.datetime.utcnow()\n",
    "print(\"Completed : \", end.strftime(\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "print(\"Time taken : \", (end - start))\n",
    "spark.sql(\"SELECT * FROM v_input_df\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d8b64b8-27ed-4181-8b9a-f60bb9d84d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PYSPARK CODE\n",
    "from pyspark.sql.functions import udf,broadcast, col, pandas_udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType,IntegerType,DoubleType, FloatType\n",
    "from rapidfuzz import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "# Python UDF\n",
    "def match_string(s1, s2):\n",
    "    val = fuzz.token_sort_ratio(s1, s2)\n",
    "    return val\n",
    "\n",
    "spark.udf.register(\"fuzztest1\",  match_string,FloatType())\n",
    "\n",
    "# Pandas UDF\n",
    "@pandas_udf(FloatType())\n",
    "def fuzz_pandas_udf(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a.combine(b, lambda x, y: fuzz.token_sort_ratio(x, y))\n",
    "\n",
    "spark.udf.register(\"fuzztest2\",  fuzz_pandas_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf9f7ed8-8e8f-4b7f-8cf8-7d1a3c3c8890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Select distinct cities from the input file/table\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_distinct_input_names \n",
    "AS \n",
    "SELECT DISTINCT name as Input_name FROM v_input_df;\n",
    "\n",
    "-- cross join the distinct cities with the reference table and broadcast the smaller table\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_crossjoin \n",
    "AS \n",
    "SELECT /*+ BROADCAST(v_distinct_input_names) */\n",
    "ref_name,  Input_name FROM DIM_CUST LEFT JOIN v_distinct_input_names;\n",
    "\n",
    "--Partition the table to 24\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_fuzzy0 AS\n",
    "SELECT /*+ REPARTITION(24) */ * FROM v_crossjoin;\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_fuzzy1 AS\n",
    "SELECT ref_name, Input_name, fuzztest1(ref_name,Input_name) AS similarity_score FROM vw_fuzzy0;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_fuzzy2 AS \n",
    " SELECT ref_name, Input_name, similarity_score\n",
    " FROM\n",
    " (\n",
    "     SELECT ROW_NUMBER() OVER (PARTITION BY input_name ORDER BY similarity_score DESC) as RowNum, *\n",
    "     FROM vw_fuzzy1\n",
    " ) \n",
    " WHERE RowNum = 1;\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_fuzzy3 AS\n",
    "SELECT input_name, ref_name, similarity_score FROM v_fuzzy2;\n",
    "\n",
    "SELECT * FROM v_fuzzy3 order by similarity_score DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68f02512-d217-4daa-ae6c-e9f84c8bd359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fuzzy_match",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
